{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Duration\n",
    "Tralala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipynb\n",
    "data_dir = Path('../data' )\n",
    "output_dir = Path('../output') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SL_daily_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extracting the necessary data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m flood_day \u001b[38;5;241m=\u001b[39m \u001b[43mSL_daily_max\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflood_day\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m time \u001b[38;5;241m=\u001b[39m SL_daily_max[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m station_names \u001b[38;5;241m=\u001b[39m SL_daily_max[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SL_daily_max' is not defined"
     ]
    }
   ],
   "source": [
    "# Extracting the necessary data\n",
    "flood_day = SL_daily_max['flood_day']\n",
    "time = SL_daily_max['time']\n",
    "station_names = SL_daily_max['station_name'].values\n",
    "\n",
    "# Convert time to pandas datetime\n",
    "time = pd.to_datetime(time.values)\n",
    "\n",
    "# Initialize a dictionary to hold results\n",
    "flood_events = {}\n",
    "\n",
    "# Loop through each station\n",
    "for i, station in enumerate(station_names):\n",
    "    station_flood_days = flood_day[:, i].values\n",
    "    station_flood_days = pd.Series(station_flood_days, index=time)\n",
    "    \n",
    "    # Group by year\n",
    "    station_flood_days_by_year = station_flood_days.groupby(station_flood_days.index.year)\n",
    "    \n",
    "    # Initialize list to hold all events for this station\n",
    "    station_events = []\n",
    "    \n",
    "    for year, data in station_flood_days_by_year:\n",
    "        flood_event_durations = []\n",
    "        current_event_length = 0\n",
    "        \n",
    "        for day in data:\n",
    "            if day:\n",
    "                current_event_length += 1\n",
    "            else:\n",
    "                if current_event_length > 0:\n",
    "                    flood_event_durations.append(current_event_length)\n",
    "                current_event_length = 0\n",
    "        \n",
    "        # Append the last event if it was ongoing at the end of the year\n",
    "        if current_event_length > 0:\n",
    "            flood_event_durations.append(current_event_length)\n",
    "        \n",
    "        # Store the events for this year\n",
    "        station_events.append({\n",
    "            \"year\": year,\n",
    "            \"events\": flood_event_durations\n",
    "        })\n",
    "    \n",
    "    # Store the results for the station\n",
    "    flood_events[station] = station_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the plot grid again with larger station names\n",
    "\n",
    "# Determine unique years in the dataset\n",
    "years = sorted(set(year_data[\"year\"] for station_events in flood_events.values() for year_data in station_events))\n",
    "\n",
    "# Setting up the plot grid\n",
    "num_stations = len(station_names)\n",
    "# Determine the 5-year intervals for labeling\n",
    "five_year_ticks = [year for year in years if year % 5 == 0]\n",
    "\n",
    "# Setting up the plot grid again with individual subplots and station names inside the plots\n",
    "fig, axes = plt.subplots(nrows=num_stations, ncols=1, figsize=(6, 10), sharex=True)\n",
    "\n",
    "# Plotting for each station\n",
    "for ax, station in zip(axes, station_names):\n",
    "    yearly_durations = {year: [] for year in years}\n",
    "    \n",
    "    # Populate the yearly durations\n",
    "    for year_data in flood_events[station]:\n",
    "        year = year_data[\"year\"]\n",
    "        yearly_durations[year].extend(year_data[\"events\"])\n",
    "    \n",
    "    # Prepare data for boxplot\n",
    "    boxplot_data = [yearly_durations[year] for year in years]\n",
    "    \n",
    "    # Create the boxplot\n",
    "    ax.boxplot(\n",
    "        boxplot_data, \n",
    "        positions=range(len(years)),\n",
    "        patch_artist=True, \n",
    "        boxprops=dict(facecolor='lightblue', color='blue', alpha=0.7),\n",
    "        medianprops=dict(color='red'),\n",
    "        whiskerprops=dict(color='blue', alpha=0.5),\n",
    "        capprops=dict(color='blue', alpha=0.5)\n",
    "    )\n",
    "    \n",
    "    # Add station name inside the plot\n",
    "    ax.text(2, ax.get_ylim()[1] * 0.8, station, fontsize=12, ha='left', color='black')\n",
    "    ax.set_ylabel('Duration (days)', fontsize=10)\n",
    "    ax.grid(False)\n",
    "\n",
    "# Clean up and remove x-axis labels from all but the last subplot\n",
    "for ax in axes[:-1]:\n",
    "    ax.set_xticks([])\n",
    "\n",
    "# Set x-ticks and labels for every 5 years\n",
    "axes[-1].set_xticks([years.index(year) for year in five_year_ticks])\n",
    "axes[-1].set_xticklabels(five_year_ticks, ha='right')\n",
    "axes[-1].set_xlabel('Year', fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each sensor column\n",
    "for column in flood_hour_df.columns:\n",
    "    flood_duration_day[column] = []\n",
    "    \n",
    "    # Find the indices where flooding starts and ends\n",
    "    flood_events = df_flood[column].ne(df_flood[column].shift()).cumsum()\n",
    "    \n",
    "    # Group by the flood event indices, filtering out False events\n",
    "    for event_id, group in df_flood.groupby(flood_events):\n",
    "        if group[column].iloc[0]:  # Only consider True (flood) events\n",
    "            start_time = group.index.min()\n",
    "            end_time = group.index.max()\n",
    "            duration = end_time - start_time \n",
    "            # round to nearest hour\n",
    "            duration = np.round(duration.total_seconds() / 3600) + 1\n",
    "            flood_durations[column].append((start_time, duration))\n",
    "            height = flood_data_df.loc[start_time:end_time, column].max() # max height\n",
    "            flood_heights[column].append((start_time, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flood_hour = (hourly_data.sea_level_MHHW.values > thresholds[:,None])\n",
    "flood_hour = (hourly_data.sea_level_MHHW.values > 30)\n",
    "flood_hour\n",
    "flood_hour = np.transpose(flood_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_hour_df = pd.DataFrame(flood_hour, index = hourly_data.time.values, columns = hourly_data.record_id.values)\n",
    "\n",
    "flood_hour_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sea level data where flood_hour is True\n",
    "\n",
    "flood_data = hourly_data.sea_level_MHHW.where(flood_hour.T)\n",
    "flood_data_df = pd.DataFrame(flood_data.T-30, index = hourly_data.time.values, columns = hourly_data.record_id.values)\n",
    "flood_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of flood events, and the duration of each event for record id = 57\n",
    "# Initialize a dictionary to store the durations\n",
    "flood_durations = {}\n",
    "flood_heights = {}\n",
    "\n",
    "\n",
    "df_flood = flood_hour_df \n",
    "\n",
    "# Loop through each sensor column\n",
    "for column in flood_hour_df.columns:\n",
    "    flood_durations[column] = []\n",
    "    flood_heights[column] = []\n",
    "    \n",
    "    # Find the indices where flooding starts and ends\n",
    "    flood_events = df_flood[column].ne(df_flood[column].shift()).cumsum()\n",
    "    \n",
    "    # Group by the flood event indices, filtering out False events\n",
    "    for event_id, group in df_flood.groupby(flood_events):\n",
    "        if group[column].iloc[0]:  # Only consider True (flood) events\n",
    "            start_time = group.index.min()\n",
    "            end_time = group.index.max()\n",
    "            duration = end_time - start_time \n",
    "            # round to nearest hour\n",
    "            duration = np.round(duration.total_seconds() / 3600) + 1\n",
    "            flood_durations[column].append((start_time, duration))\n",
    "            height = flood_data_df.loc[start_time:end_time, column].max() # max height\n",
    "            flood_heights[column].append((start_time, height))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe from dictionary\n",
    "# Assuming flood_durations and hourly_data are already defined\n",
    "\n",
    "# Initialize an empty list to store DataArrays\n",
    "data_arrays = []\n",
    "\n",
    "# Loop through each record_id and create a DataArray\n",
    "for record_id in hourly_data.record_id.values:\n",
    "    flood_durations_df = pd.DataFrame(flood_durations[record_id])\n",
    "    flood_durations_df.columns = ['time', 'duration']\n",
    "\n",
    "    flood_heights_df = pd.DataFrame(flood_heights[record_id])   \n",
    "    flood_heights_df.columns = ['time', 'height']\n",
    "    \n",
    "    # Merge the two DataFrames on 'time'\n",
    "    merged_df = pd.merge(flood_durations_df, flood_heights_df, on='time')\n",
    "    \n",
    "    # Create a DataArray with both 'duration' and 'height'\n",
    "    flood_data_da = xr.DataArray(\n",
    "        merged_df[['duration', 'height']].values,\n",
    "        dims=['time', 'variable'],\n",
    "        coords={'time': merged_df.time.values, 'variable': ['duration', 'height'], 'record_id': record_id}\n",
    "    )\n",
    "    \n",
    "    data_arrays.append(flood_data_da)\n",
    "\n",
    "\n",
    "# Combine all DataArrays into a single Dataset\n",
    "flood_data_ds = xr.concat(data_arrays, dim='record_id')\n",
    "\n",
    "# Convert to Dataset\n",
    "flood_data_ds = flood_data_ds.to_dataset(dim='variable')\n",
    "\n",
    "flood_data_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make storm time by shifting time by 4 months, such that May 1st is the start of the storm season\n",
    "# Convert the time values to pandas DatetimeIndex\n",
    "time_values = pd.to_datetime(flood_data_ds.time.values)\n",
    "\n",
    "# Jan 1 should be the equivalent of May 1\n",
    "shifted_time_values = time_values + pd.DateOffset(months=4)\n",
    "\n",
    "#remove anything in 1982\n",
    "shifted_time_values = shifted_time_values[shifted_time_values.year != 1982]\n",
    "\n",
    "\n",
    "# Assign the shifted time values back to the dataset\n",
    "flood_data_ds['time'] = shifted_time_values\n",
    "\n",
    "\n",
    "flood_data_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot flood duration vs year\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# make figure size small\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Create a custom color ramp and exclude the lightest shade\n",
    "full_palette = sns.color_palette(\"Blues\", n_colors=256)\n",
    "# adjusted_heatmap_palette = full_palette[50:]  # Skip the lightest 50 shades\n",
    "\n",
    "# Convert the adjusted palette to a colormap\n",
    "# custom_palette = LinearSegmentedColormap.from_list(\"custom_blues\", adjusted_heatmap_palette)\n",
    "custom_palette = sns.color_palette(\"OrRd\", as_cmap=True)\n",
    "\n",
    "\n",
    "# selected_data['duration'] = selected_data['duration'].astype(float)\n",
    "# flood_durations_ds['month'] = flood_durations_ds['month'].astype(float)\n",
    "\n",
    "record_ids = flood_data_ds['record_id'].values\n",
    "station_names = hourly_data['station_name'].sel(record_id=record_ids).values\n",
    "\n",
    "for i, record_id in enumerate(record_ids):\n",
    "    # Select data for the current record_id and drop NaNs\n",
    "    selected_data = flood_data_ds.sel(record_id=record_id).dropna(dim='time')\n",
    "    \n",
    "    # Ensure 'duration' is numeric\n",
    "    selected_data['duration'] = selected_data['duration'].astype(float)\n",
    "    \n",
    "    # Plot each station's data on a different y-coordinate\n",
    "    scatter = plt.scatter(selected_data['time'], [i + 1] * len(selected_data['time']), s=10 * selected_data['duration'], c=selected_data['height'], cmap=custom_palette)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.yticks(range(1, len(record_ids) + 1), hourly_data['station_name'].sel(record_id=record_ids).values)  # Set y-ticks to show station labels\n",
    "plt.colorbar(scatter, label='Max height \\nabove threshold (cm)')\n",
    "\n",
    "# show the colorbar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLI39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of Sea Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually stuff will go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, I am stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following Merrifield, Firing & Marra:\n",
    "This technique uses the Peaks Over Treshold (POT) technique to obtain extrema. This treshold is based on annual climatologies, obtained with the following steps:\n",
    "\n",
    "1. Obtain hourly and daily mean tide gauge data from UHSLC\n",
    "2. Deconstruct the time series into components with different timescales\n",
    "3. Get high-pass filtered data:\n",
    "    1. Remove linear trend\n",
    "    2. Obtain low-frequency SL variability by smoothing daily data with a running Gaussian-shaped low-pass filter (half power period at ~5 months).\n",
    "    3. Compute seasonal cycle by averaging daily mean WL over all years for each year day\n",
    "    4. High-frequency SL is the residual of the daily means after removing #3,#4,#5\n",
    "4. Tidal component is predicted with fits to hourly data\n",
    "5. Reference all data to MHHW\n",
    "6. Extreme events are the superposition of seasonal sea level, tidal and high frequency water level components.\n",
    "    1. Tidal contribution is 95% exceedence threshold of daily highest water above MHHW on each day.\n",
    "    2. HF variability is the 95% exceedance threshold of the HF filtered data on each year day.\n",
    "7. Obtain TWL (H95) by stacking the seasonal sea level, 95% tide contribution, and 95% HF contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconstruct the time series into components with different timescales.\n",
    "\n",
    "### Get high-pass filtered data\n",
    "#### Remove linear trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_trend_with_nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# remove the linear trend from rsl_daily\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m trend_mag, trend_line, trend_rate \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_trend_with_nan\u001b[49m(rsl_daily[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsl_mhhw\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m rsl_daily[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsl_mhhw_detrended\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m rsl_daily[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsl_mhhw\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m trend_line\n\u001b[0;32m      7\u001b[0m rsl_daily[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsl_mhhw_detrended\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msel(record_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m590\u001b[39m)\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_trend_with_nan' is not defined"
     ]
    }
   ],
   "source": [
    "# remove the linear trend from rsl_daily\n",
    "\n",
    "trend_mag, trend_line, trend_rate = process_trend_with_nan(rsl_daily['rsl_mhhw'])\n",
    "\n",
    "rsl_daily['rsl_mhhw_detrended'] = rsl_daily['rsl_mhhw'] - trend_line\n",
    "\n",
    "rsl_daily['rsl_mhhw_detrended'].sel(record_id = 590).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain low-frequency SL variability \n",
    "We'll do this by smoothing daily data with a running Gaussian-shaped low-pass filter (half power period at ~5 months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# Define the filter parameters\n",
    "half_power_period = 5 * 30  # Approximate number of days in 5 months\n",
    "sigma = half_power_period / np.sqrt(8 * np.log(2))  # Convert half-power period to standard deviation\n",
    "\n",
    "# Detrend each station's time series and handle NaNs\n",
    "rsl_mhhw_detrended = np.empty_like(rsl_daily['rsl_mhhw'].values)\n",
    "\n",
    "for i in range(rsl_daily['record_id'].size):\n",
    "    data = rsl_daily['rsl_mhhw'][i, :].values\n",
    "    mask = np.isnan(data)\n",
    "    data[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), data[~mask])\n",
    "    rsl_mhhw_detrended[i, :] = detrend(data)\n",
    "\n",
    "\n",
    "# Apply the Gaussian filter to the detrended data\n",
    "rsl_mhhw_detrended_filtered = gaussian_filter1d(rsl_mhhw_detrended, sigma=sigma, axis=1)\n",
    "\n",
    "# Add the detrended and filtered data to the dataset\n",
    "rsl_daily['rsl_mhhw_detrended_interp'] = (('record_id', 'time'), rsl_mhhw_detrended)\n",
    "rsl_daily['rsl_mhhw_detrended_filtered'] = (('record_id', 'time'), rsl_mhhw_detrended_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the seasonal cycle by averaging daily mean water levels over all years for each year day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute seasonal cycle by averaging daily rsl_mhhw over all years for each year day\n",
    "seasonal_cycle = rsl_daily['rsl_mhhw_detrended'].groupby('time.dayofyear').mean(dim='time') \n",
    "\n",
    "# Plot the seasonal cycle, with 11 different lines, and add the station names to the legend\n",
    "plt.figure()\n",
    "for i in range(seasonal_cycle.shape[0]):\n",
    "    plt.plot(seasonal_cycle['dayofyear'], seasonal_cycle[i, :], label=rsl_daily['station_name'].values[i], alpha=0.5)\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Median Detrended rsl_mhhw [m]')\n",
    "plt.title('Seasonal Cycle of rsl_mhhw Detrended')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain High Frequency data\n",
    "It is the residual of the daily means after removing the linear trend, low-frequency variability, and seasonal cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the seasonal cycle to the full time series length for each station\n",
    "seasonal_cycle_full = seasonal_cycle.sel(dayofyear=rsl_daily['time'].dt.dayofyear)\n",
    "\n",
    "# Compute the residual by removing the linear trend, filtered data, and seasonal cycle\n",
    "residual = rsl_daily['rsl_mhhw_detrended'] - rsl_mhhw_detrended_filtered - seasonal_cycle_full\n",
    "\n",
    "# Add the residual to the dataset\n",
    "rsl_daily['rsl_mhhw_high_pass_filtered'] = (('record_id', 'time'), residual.values)\n",
    "\n",
    "# Plot the residual for each station\n",
    "plt.figure()\n",
    "for i in range(rsl_daily['record_id'].size):\n",
    "    rsl_daily['rsl_mhhw_high_pass_filtered'].sel(record_id=rsl_daily['record_id'][i]).plot(label=rsl_daily['station_name'].values[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidal component is predicted with fits to hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_continuous_segments(time, data, max_gap_hours=12):\n",
    "    # Create a DataFrame from the time and data arrays\n",
    "    df = pd.DataFrame({'time': time, 'data': data})\n",
    "    \n",
    "    # Calculate the time difference in hours\n",
    "    df['time_diff'] = df['time'].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    # Identify the start of new segments\n",
    "    df['segment'] = (df['time_diff'] > max_gap_hours).cumsum()\n",
    "    \n",
    "    # Group by the segment identifier and convert groups to list of tuples\n",
    "    segments = [list(zip(group['time'], group['data'])) for _, group in df.groupby('segment')]\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time in rsl_hourly to fractional year, starting at 0\n",
    "time = rsl_hourly['time']\n",
    "time = time - time[0]\n",
    "time = time/np.timedelta64(1, 'D')\n",
    "time = time/365.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure that data_dir/rsl_hawaii_tidal_predictions.nc exists, if not, run the following code to create it\n",
    "if not (data_dir / 'rsl_hawaii_tidal_predictions.nc').exists():\n",
    "    print('rsl_hawaii_tidal_predictions.nc not found in ../../data. Will proceed with tidal analysis and prediction.')\n",
    "\n",
    "\n",
    "    # Prepare an empty array to store tidal predictions for all stations\n",
    "    sea_level_shape = rsl_hourly['sea_level'].shape\n",
    "    tidal_predictions = np.full(sea_level_shape, np.nan)  # Initialize with NaNs\n",
    "\n",
    "    # Perform tidal analysis and prediction for each station\n",
    "    for i, station_id in enumerate(rsl_hourly['record_id'].values):\n",
    "        print('Processing ' + str(rsl_hourly['station_name'].sel(record_id=station_id).values))\n",
    "\n",
    "        sea_level_data = rsl_hourly['sea_level'].sel(record_id=station_id).values\n",
    "        #remove linear trend\n",
    "        sea_level_data = sea_level_data - trend_rate.sel(record_id=station_id).values*1000*time\n",
    "        # print the % nans in the data\n",
    "        print('Percentage of NaNs in the data: ' + str(np.isnan(sea_level_data).sum() / len(sea_level_data) * 100) + '%')\n",
    "        time_data = rsl_hourly['time'].values\n",
    "        latitude = rsl_hourly['lat'].sel(record_id=station_id).values\n",
    "\n",
    "        # Convert time_data to pandas datetime format for UTide\n",
    "        time_series = pd.to_datetime(time_data)\n",
    "\n",
    "        # Split data into continuous segments\n",
    "        segments = split_continuous_segments(time_series, sea_level_data)\n",
    "        print('Number of segments: ' + str(len(segments)))\n",
    "\n",
    "        # Perform harmonic analysis on each segment\n",
    "        for segment in segments:\n",
    "            # print counter\n",
    "            print('Processing segment ' + str(segments.index(segment) + 1) + ' of ' + str(len(segments)))\n",
    "            segment_time, segment_data = zip(*segment)\n",
    "            segment_time = np.array(segment_time)\n",
    "            segment_data = np.array(segment_data)\n",
    "    \n",
    "            # Convert datetime to numeric format for interpolation\n",
    "            segment_time_numeric = segment_time.astype('datetime64[s]').astype(np.float64)\n",
    "    \n",
    "            # Check for NaNs and interpolate to fill NaNs if needed\n",
    "            if np.isnan(segment_data).any():\n",
    "                mask = np.isnan(segment_data)\n",
    "                interp_func = interp1d(segment_time_numeric[~mask], segment_data[~mask], kind='linear', fill_value=\"extrapolate\")\n",
    "                segment_data[mask] = interp_func(segment_time_numeric[mask])\n",
    "    \n",
    "            # Perform harmonic analysis\n",
    "            coef = utide.solve(segment_time, segment_data, lat=latitude)\n",
    "    \n",
    "            # Predict the tide using the fitted model\n",
    "            tide_pred = utide.reconstruct(segment_time, coef)\n",
    "    \n",
    "            # Store the tidal predictions in the array\n",
    "            for t, pred in zip(segment_time, tide_pred.h):\n",
    "                idx = np.where(time_series == t)[0][0]\n",
    "                tidal_predictions[i, idx] = pred\n",
    "    \n",
    "    # Create the tidal predictions xarray Dataset with the same structure as rsl_hourly\n",
    "    tidal_predictions_ds = xr.Dataset(\n",
    "        data_vars={'tidal_prediction': (('record_id', 'time'), tidal_predictions)},\n",
    "        coords={'time': rsl_hourly['time'].values, 'record_id': rsl_hourly['record_id'].values}\n",
    "    )\n",
    "\n",
    "    # save rsl_daily_combined to the data directory\n",
    "    tidal_predictions_ds.to_netcdf(data_dir / 'rsl_hawaii_tidal_predictions.nc')\n",
    "    print(f'Tidal predictions saved to: {data_dir / \"rsl_hawaii_tidal_predictions.nc\"}')\n",
    "\n",
    "else:\n",
    "    print('rsl_hawaii_tidal_predictions.nc found in ../../data. Proceed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference all data to MHHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open rsl_hawaii_tidal_predictions.nc\n",
    "rsl_hawaii_tidal_predictions = xr.open_dataset(data_dir / 'rsl_hawaii_tidal_predictions.nc')\n",
    "#\n",
    "\n",
    "rsl_hawaii_tidal_predictions\n",
    "#Adjust the tidal precictions to be in MHHW\n",
    "# for station_id in rsl_hawaii_tidal_predictions.data_vars:\n",
    "    # rsl_hawaii_tidal_predictions[station_id] = rsl_hawaii_tidal_predictions[station_id] + rsl_hourly['MHHW'].sel(record_id=int(station_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference it to MHHW\n",
    "rsl_hawaii_tidal_predictions['tidal_prediction_mhhw_m'] = 0.001*(rsl_hawaii_tidal_predictions['tidal_prediction'] - rsl_hourly['MHHW'])\n",
    "\n",
    "#plot the tidal predictions for Honolulu\n",
    "rid = 4\n",
    "rsl_hourly['sea_level_mhhw'] = 0.001*(rsl_hourly['sea_level'] - rsl_hourly['MHHW'])\n",
    "rsl_hourly['sea_level_mhhw'].sel(record_id=rsl_daily.record_id[rid]).plot()\n",
    "# rsl_daily['rsl_mhhw_detrended_interp'].sel(record_id=rsl_daily.record_id[rid]).plot()\n",
    "rsl_hawaii_tidal_predictions['tidal_prediction_mhhw_m'].sel(record_id=rsl_daily.record_id[rid]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find 95% exceedence threshold for tidal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"The tidal contribution is the 95% exceedance threshold of daily highest water above the MHHW on each year-day.\"\"\n",
    "\n",
    "# first we'll get \"daily highest water\"\n",
    "rsl_daily_max_tidal = rsl_hawaii_tidal_predictions['tidal_prediction_mhhw_m']\n",
    "# .groupby('time.day').max(dim='time')\n",
    "#plot\n",
    "# get daily max tidal prediction by resampling to daily and taking the max\n",
    "rsl_daily_max_tidal = rsl_daily_max_tidal.resample(time='1D').max(dim='time')\n",
    "\n",
    "#If we go by the exact wording in the Merrifield doc, it's the 95th percentile of the daily max tidal prediction\n",
    "# and we need to do this on year day\n",
    "# seasonal_cycle = rsl_daily['rsl_mhhw_detrended'].groupby('time.dayofyear').median(dim='time') #NOTE I AM USING MEDIAN!!!\n",
    "\n",
    "# # get 95th percentile of rsl_daily_max_tidal\n",
    "# rsl_daily_max_tidal_95 = rsl_daily_max_tidal.quantile(0.95, dim='time')\n",
    "\n",
    "# rsl_daily_max_tidal_95\n",
    "\n",
    "# rsl_daily_max_tidal_95 = rsl_daily_max_tidal.groupby('time.dayofyear').quantile(0.95, dim='time')\n",
    "# rsl_daily_max_tidal_50 = rsl_daily_max_tidal.groupby('time.dayofyear').quantile(0.5, dim='time')\n",
    "# rsl_daily_max_tidal_5 = rsl_daily_max_tidal.groupby('time.dayofyear').quantile(0.05, dim='time')\n",
    "\n",
    "\n",
    "# rsl_daily_max_tidal_95\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HF variability is the 95% exceedance threshold of the HF filtered data on each year day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 95th percentile of the HF residual data\n",
    "rsl_daily_residual_95 = rsl_daily['rsl_mhhw_high_pass_filtered'].groupby('time.dayofyear').quantile(0.95, dim='time')\n",
    "\n",
    "rsl_daily_residual_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsl_low_frequency = rsl_daily['rsl_mhhw_detrended_filtered'].sel(record_id=rsl_daily.record_id[rid]) + trend_line.sel(record_id=rsl_daily.record_id[rid])\n",
    "\n",
    "# plot rsl_low_frequency, trend_line\n",
    "rsl_low_frequency.plot()\n",
    "trend_line.sel(record_id=rsl_daily.record_id[rid]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain TWL (H95) by stacking the seasonal sea level, tide contribution, and HF contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H95 = seasonal_cycle+ rsl_daily_max_tidal_95+ rsl_daily_residual_95\n",
    "H95.sel(record_id=570).plot()\n",
    "# seasonal_cycle.sel(record_id=570).plot()\n",
    "rsl_daily_max_tidal_95.sel(record_id=570).plot()\n",
    "rsl_daily_max_tidal_5.sel(record_id=570).plot()\n",
    "\n",
    "rsl_daily_max_tidal_50.sel(record_id=570).plot()\n",
    "\n",
    "# rsl_daily_residual_95.sel(record_id=570).plot()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}